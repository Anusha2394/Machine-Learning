# -*- coding: utf-8 -*-
"""Anusha_kasa_clustering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FAfR8E5oyPdJiTKhun-biwrHTo7B1WVB

Imporing libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import KMeans

"""Reading Traing data"""

df_train=pd.read_csv('/content/ALS_TrainingData_2223.csv')

df_train.head()

"""Checking for columns present in training data"""

df_train.columns

df_train.shape

"""Checking for null values"""

df_train.isnull().sum()

"""As we donot get deteiled information we check for null values using heatmap"""

sns.heatmap(df_train.isnull(),yticklabels=False,cbar=False)

"""Finding for correlations in data"""

sns.heatmap(df_train.corr())

"""Analyzing Numeical variables in data"""

df_train.describe()

"""As there are many features we perform feature selection to select best features that influence ALSFRS_slope"""

X = df_train.drop(['ALSFRS_slope'],axis=1)

y=df_train['ALSFRS_slope']

"""Performing feature selection and finding 10 best features"""

bestfeatures = SelectKBest(score_func=f_classif, k=10)
fit = bestfeatures.fit(X,y)
dfscores = pd.DataFrame(fit.scores_)
dfcolumns = pd.DataFrame(X.columns)
#concat two dataframes for better visualization 
featureScores = pd.concat([dfcolumns,dfscores],axis=1)
featureScores.columns = ['Best_features','Score']  #naming the dataframe columns
print(featureScores.nlargest(10,'Score'))

"""Plotting importance of top 10 features"""

selection = ExtraTreesRegressor()
selection.fit(X, y)
plt.figure(figsize = (12,8))
feat_importances = pd.Series(selection.feature_importances_, index=X.columns)
feat_importances.nlargest(10).plot(kind='barh')
plt.show()

cols=featureScores.nlargest(10,'Score')['Best_features'].values
cols

"""Preparing a new dataframe with top features to train model"""

df_new_cols=['ALSFRS_Total_range', 'trunk_range', 'hands_range', 'leg_range',
       'ALSFRS_Total_min', 'Albumin_range', 'mouth_range',
       'bp_diastolic_range', 'ALT.SGPT._range', 'trunk_min','ALSFRS_slope']
df_new=df_train[df_new_cols]
df_new.head()

"""Visualizing slope parameter"""

sns.boxplot( y=df_new["ALSFRS_slope"] )

"""Visualizing the top required features"""

sns.pairplot(df_new)

"""As the values are so large we scale the features using MinMaxScalar"""

scaler=MinMaxScaler()
df_new[['ALSFRS_Total_min', 'trunk_min']] = scaler.fit_transform(df_new[['ALSFRS_Total_min','trunk_min']].to_numpy())
df_new.head()

"""Reading Testing data"""

df_test = pd.read_csv('/content/ALS_TestingData_78.csv')

df_test.head()

df_test.shape

"""Preparing dataframe for testing data using top 10 columns"""

df_test_cols=['ALSFRS_Total_range', 'trunk_range', 'hands_range', 'leg_range',
       'ALSFRS_Total_min', 'Albumin_range', 'mouth_range',
       'bp_diastolic_range', 'ALT.SGPT._range', 'trunk_min','ALSFRS_slope']

df_test_new=df_test[df_test_cols]
df_test_new.head()

"""Appling MinMax scalar for Testing data"""

df_test_new[['ALSFRS_Total_min', 'trunk_min']] = scaler.fit_transform(df_test_new[['ALSFRS_Total_min','trunk_min']].to_numpy())
df_test_new.head()

"""Training Model"""

X_train=df_new.drop(['ALSFRS_slope'],axis=1)
X_test=df_test_new.drop(['ALSFRS_slope'],axis=1)

"""fitting Model for Multiple k values from 1 to 11 """

kmeans_kwargs = {
"init": "random",
"n_init": 10,
"max_iter": 300,
"random_state": 42,
}
sse=[]
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)
    kmeans.fit(X_train)
    y_pred=kmeans.predict(X_test)
    print(y_pred)
    sse.append(kmeans.inertia_)

"""Plotting Elbow curve to find best k value"""

plt.style.use("fivethirtyeight")
plt.plot(range(1, 11), sse)
plt.xticks(range(1, 11))
plt.xlabel("Number of Clusters")
plt.ylabel("SSE")
plt.show()

"""From the curve it is clear that k=3 is best to train the model so we print the results of prediction and cluster centers"""

kmeans = KMeans(n_clusters=3, **kmeans_kwargs)
kmeans.fit(X_train)
y_pred=kmeans.predict(X_test)
print(y_pred)
print(kmeans.cluster_centers_)

"""visualizing cluster centers and formation of clusters """

centers = kmeans.cluster_centers_
plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred, s=50, cmap='viridis');
plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);

