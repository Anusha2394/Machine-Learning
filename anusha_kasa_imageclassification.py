# -*- coding: utf-8 -*-
"""Anusha_Kasa_imageClassification

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GFf4hOB9YIRAa7xtB7yODY8L10Pq4vkU

Enabling gpu
"""

import tensorflow as tf
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))



"""Mounting drive"""

from google.colab import drive
drive.mount('/content/drive')

"""Importing libraries"""

import pandas as pd
import numpy as np

"""calling dataset"""

train="/content/drive/MyDrive/seg_train.zip"
test="/content/drive/MyDrive/seg_test.zip"
pred="/content/drive/MyDrive/seg_pred.zip"

import os

!unzip -uq "/content/drive/MyDrive/seg_train.zip" -d "drive/My Drive/train_data"

!unzip -uq "/content/drive/MyDrive/seg_test.zip" -d "drive/My Drive/test_data"

!unzip -uq "/content/drive/MyDrive/seg_pred.zip" -d "drive/My Drive/pred_data"

"""Importing libraries"""

import matplotlib.pyplot as plt
import matplotlib.image as implt
from PIL import Image
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

train_dataset='/content/drive/MyDrive/train_data/seg_train'
test_dataset='/content/drive/MyDrive/test_data/seg_test'

train_buildings='/content/drive/MyDrive/train_data/seg_train/buildings'
train_forest='/content/drive/MyDrive/train_data/seg_train/forest'
train_glacier='/content/drive/MyDrive/train_data/seg_train/glacier'
train_mountain='/content/drive/MyDrive/train_data/seg_train/mountain'
train_sea='/content/drive/MyDrive/train_data/seg_train/sea'
train_street='/content/drive/MyDrive/train_data/seg_train/street'

"""defining arrays"""

img_size=50
buildings_train=[]
forest_train=[]
glacier_train=[]
mountain_train=[]
sea_train=[]
street_train=[]

"""labelling data"""

buildings_labels=np.zeros(2191)
forest_labels=np.ones(2271)
glacier_labels=np.full((2404),2)
mountain_labels=np.full((2512),3)
sea_labels=np.full((2274),4)
street_labels=np.full((2382),5)

"""converting image into grey scale
resizing image
convering image in to bit format for images
"""

for i in os.listdir(train_buildings):
     if os.path.isfile(train_dataset + "/buildings/" + i):
            buildings=Image.open(train_dataset+'/buildings/'+i).convert('L') 
            buildings=buildings.resize((img_size,img_size),Image.ANTIALIAS) 
            buildings=np.asarray(buildings)
            buildings_train.append(buildings)
for i in os.listdir(train_forest):
     if os.path.isfile(train_dataset + "/forest/" + i):
            forest=Image.open(train_dataset+'/forest/'+i).convert('L') 
            forest=forest.resize((img_size,img_size),Image.ANTIALIAS) 
            forest=np.asarray(forest)
            forest_train.append(forest)
for i in os.listdir(train_glacier):
     if os.path.isfile(train_dataset + "/glacier/" + i):
            glacier=Image.open(train_dataset+'/glacier/'+i).convert('L') 
            glacier=glacier.resize((img_size,img_size),Image.ANTIALIAS) 
            glacier=np.asarray(glacier)
            glacier_train.append(glacier)
for i in os.listdir(train_mountain):
     if os.path.isfile(train_dataset + "/mountain/" + i):
            mountain=Image.open(train_dataset+'/mountain/'+i).convert('L')
            mountain=mountain.resize((img_size,img_size),Image.ANTIALIAS) 
            mountain=np.asarray(mountain)
            mountain_train.append(mountain)
for i in os.listdir(train_sea):
     if os.path.isfile(train_dataset + "/sea/" + i):
            sea=Image.open(train_dataset+'/sea/'+i).convert('L')
            sea=sea.resize((img_size,img_size),Image.ANTIALIAS) 
            sea=np.asarray(sea)
            sea_train.append(sea)
for i in os.listdir(train_street):
     if os.path.isfile(train_dataset + "/street/" + i):
            street=Image.open(train_dataset+'/street/'+i).convert('L') 
            street=street.resize((img_size,img_size),Image.ANTIALIAS) 
            street=np.asarray(street)
            street_train.append(street)
			
X_train=np.concatenate((buildings_train,forest_train,glacier_train,mountain_train,sea_train,street_train),axis=0)
Y_train=np.concatenate((buildings_labels,forest_labels,glacier_labels,mountain_labels,sea_labels,street_labels),axis=0).reshape(X_train.shape[0],1)
print('X train shape:',X_train.shape)
print('Y train shape:',Y_train.shape)

X_train = X_train.reshape(-1,50,50,1)
print("x_train shape: ",X_train.shape)

from keras.utils.np_utils import to_categorical
Y_train = to_categorical(Y_train, num_classes = 6)

"""performing train test split"""

from sklearn.model_selection import train_test_split
X_train,X_val,Y_train,Y_val=train_test_split(X_train,Y_train,test_size=0.1,random_state=2)
print("x_train shape",X_train.shape)
print("x_test shape",X_val.shape)
print("y_train shape",Y_train.shape)
print("y_test shape",Y_val.shape)

from sklearn.metrics import confusion_matrix

import itertools

"""Performing one-hot-encoding"""

from keras.utils.np_utils import to_categorical # convert to one-hot-encoding
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D
from keras.optimizers import RMSprop,Adam
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau

"""defining model"""

model = Sequential()

model.add(Conv2D(filters = 16, kernel_size = (5,5),padding = 'Same', 
                 activation ='relu', input_shape = (50,50,1)))
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Dropout(0.25))
model.add(Conv2D(filters = 16, kernel_size = (3,3),padding = 'Same', 
                 activation ='relu'))
model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))
model.add(Dropout(0.25))
# fully connected
model.add(Flatten())
model.add(Dense(256, activation = "relu"))
model.add(Dropout(0.5))
model.add(Dense(6, activation = "softmax"))

"""defining optimizer"""

optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)

model.compile(optimizer = optimizer , loss = "categorical_crossentropy", metrics=["accuracy"])

epochs = 50 
batch_size = 250

"""As data has rto be recognized in any direction data augumentation has to be performed to flip rotate and make transformation of data and build generalized model"""

datagen = ImageDataGenerator(
        featurewise_center=False,  
        samplewise_center=False,
        featurewise_std_normalization=False, 
        samplewise_std_normalization=False,  
        zca_whitening=False, 
        rotation_range=5,  
        zoom_range = 0.1, 
        width_shift_range=0.1,  
        height_shift_range=0.1, 
        horizontal_flip=False,  
        vertical_flip=False)

datagen.fit(X_train)

"""Fitting model"""

history = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size),
                              epochs = epochs, validation_data = (X_val,Y_val), steps_per_epoch=X_train.shape[0] // batch_size)

"""Performing K-fold cross validation"""

from sklearn.model_selection import KFold

kfold = KFold(n_splits=10, shuffle=True)
cvscores = []
for train, test in kfold.split(X_train, Y_train):
      model.fit(X_train, Y_train, epochs=150, batch_size=10, verbose=0)
      scores = model.evaluate(X_val, Y_val, verbose=0)
      print("%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))
      cvscores.append(scores[1] * 100)
print("%.2f%% (+/- %.2f%%)" % (numpy.mean(cvscores), numpy.std(cvscores)))

"""confusion matrix"""

from sklearn.metrics import confusion_matrix
print(confusion_matrix(validation_generator.classes, y_pred))

"""Performing Alexnet

Importing Libraries
"""

import torch
from torch import nn
import torch.nn.functional as F
import torchvision.transforms as transforms  
import torchvision
import os
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

import cv2
import pandas as pd
import torchvision.transforms as transforms 
from torchvision.transforms import ToTensor,Normalize, RandomHorizontalFlip, Resize
from torch.utils.data import DataLoader
from torch.utils.data.sampler import SubsetRandomSampler
from torch.autograd import Variable

outcomes = os.listdir(train_dataset)

print(outcomes)

transform = torchvision.transforms.Compose([
    transforms.Resize((150,150)),
    transforms.RandomHorizontalFlip(p=0.5), # randomly flip and rotate
    transforms.ColorJitter(0.3,0.4,0.4,0.2),
    transforms.ToTensor(),
    transforms.Normalize((0.425, 0.415, 0.405), (0.205, 0.205, 0.205))
    ])

transform_tests = torchvision.transforms.Compose([
    transforms.Resize((150,150)),
    transforms.ToTensor(),
    transforms.Normalize((0.425, 0.415, 0.405), (0.255, 0.245, 0.235))
    ])

train_data = torchvision.datasets.ImageFolder(root=train_dataset,transform=transform)
test_data = torchvision.datasets.ImageFolder(root=test_dataset,transform=transform_tests)

valid_size = 0.15

num_train = len(train_data)
indices = list(range(num_train))
np.random.shuffle(indices)
split = int(np.floor(valid_size * num_train))
train_idx, valid_idx = indices[split:], indices[:split]

train_sampler = SubsetRandomSampler(train_idx)
valid_sampler = SubsetRandomSampler(valid_idx)
classes=('buildings','forest','glacier','mountain','sea','street')

train_loader = DataLoader(train_data,batch_size=50,sampler=train_sampler,num_workers=2)
valid_loader = DataLoader(train_data, batch_size =100, sampler=valid_sampler, num_workers=3)
test_loader= DataLoader(test_data,batch_size=32,shuffle=False,num_workers=2)

train_on_gpu = torch.cuda.is_available()

device =  torch.device('cuda' if torch.cuda.is_available else 'cpu')

import matplotlib.pyplot as plt
import numpy as np

def imshow(img):
    img = img / 2 + 0.5     # unnormalize
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()

dataiter = iter(train_loader)
images, labels = dataiter.next()

imshow(torchvision.utils.make_grid(images))

print(' '.join('%5s' % classes[labels[j]] for j in range(4)))

AlexNet_model = torch.hub.load('pytorch/vision:v0.6.0', 'alexnet', pretrained=True)

AlexNet_model.eval()

AlexNet_model.classifier[4] = nn.Linear(4096,1024)

AlexNet_model.classifier[6] = nn.Linear(1024,10)

AlexNet_model.eval()

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)
AlexNet_model.to(device)

import torch.optim as optim
import torch.nn as nn

criterion = nn.CrossEntropyLoss()

optimizer = optim.SGD(AlexNet_model.parameters(), lr=0.001, momentum=0.9)

for epoch in range(10):  # loop over the dataset multiple times
    running_loss = 0.0
    for i, data in enumerate(train_loader, 0):
        # get the inputs; data is a list of [inputs, labels]
        inputs, labels = data[0].to(device), data[1].to(device)

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        output = AlexNet_model(inputs)
        loss = criterion(output, labels)
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
        if i % 2000 == 1999:    # print every 2000 mini-batches
            print('[%d, %5d] loss: %.3f' %
                  (epoch + 1, i + 1, running_loss / 2000))
            running_loss = 0.0

correct = 0
total = 0

with torch.no_grad():
    for data in test_loader:
        images, labels = data[0].to(device), data[1].to(device)
        outputs = AlexNet_model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print('Accuracy of the network on the 3000 test images: %d %%' % (
    100 * correct / total))

class_correct = list(0. for i in range(10))
class_total = list(0. for i in range(10))
with torch.no_grad():
    for data in test_loader:
        images, labels = data[0].to(device), data[1].to(device)
        outputs = AlexNet_model(images)
        _, predicted = torch.max(outputs, 1)
        c = (predicted == labels).squeeze()
        for i in range(4):
            label = labels[i]
            class_correct[label] += c[i].item()
            class_total[label] += 1

for i in range(6):
    print('Accuracy of %5s : %2d %%' % (
        classes[i], 100 * class_correct[i] / class_total[i]))

avg = 0
for i in range(6):
  temp = (100 * class_correct[i] / class_total[i])
  avg = avg + temp
avg = avg/10
print('Average accuracy = ',avg)

